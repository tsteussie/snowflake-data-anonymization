import snowflake.snowpark as snowpark
import snowflake.snowpark as snowpark
from snowflake.snowpark.functions import col, upper, lower, trim, lit, current_timestamp, when, count, sum as sf_sum, coalesce
from snowflake.snowpark import Session
from functools import reduce
from collections import defaultdict as dd

email_submitted = "test@gmail.com"

# The following databases were not identified in historic data-anonymization requests: 'DEV_VAULT','PRD_VAULT','TST_VAULT','HANDLED','HUBSPOT_7508023'
search_databases = ["DEV_COPY", "DEV_DBT", "DEV_INFODEPRC", "DEV_LAKE", "DEV_MART", "DEV_MART_STAGE", "DEV_WORK", \
                    "PRD_COPY", "PRD_DBT", "PRD_INFODEPRC", "PRD_LAKE", "PRD_MART_STAGE", "PRD_WORK", \
                    "TST_COPY", "TST_DB" , "TST_INFODEPRC", "TST_LAKE", "TST_MART", "TST_MART_STAGE", "TST_WORK", \
                    "DEVELOPMENT", "PRODUCTION"]

connection_parameters = {
    "role": "SYSADMIN",
    "warehouse": "DNA_WH"
}

# -----------------------------------------------------------------------------
# SECTION 1: SPECIFY INCLUSION & EXCLUSION CRITERIA USING PYTHON DSL
# -----------------------------------------------------------------------------

# Primary Filter Conditions
filter_cond = (
       (col("COLUMN_NAME") == "NAME")
    |  (col("COLUMN_NAME") == "COMPANY")
    |  (col("COLUMN_NAME") == "SSN")
    |  (col("COLUMN_NAME") == "PRINCIPAL")
    |  upper(col("COLUMN_NAME")).like("%SSN_%")
    |  upper(col("COLUMN_NAME")).like("%_SSN%")
    |  upper(col("COLUMN_NAME")).like("%DATE_OF_BIRTH%")
    |  upper(col("COLUMN_NAME")).like("%NAME_FRST%")
    |  upper(col("COLUMN_NAME")).like("%NAME_FIRST%")
    |  upper(col("COLUMN_NAME")).like("%FRST_NAME%")
    |  upper(col("COLUMN_NAME")).like("%FIRST_NAME%")
    |  upper(col("COLUMN_NAME")).like("%NAME_LAST%")
    |  upper(col("COLUMN_NAME")).like("%LAST_NAME%")
    |  upper(col("COLUMN_NAME")).like("%LEGAL_NAME%")
    |  upper(col("COLUMN_NAME")).like("%ATTN_NAME%")
    |  upper(col("COLUMN_NAME")).like("%DBA_NAME%")
    |  upper(col("COLUMN_NAME")).like("%_INIT%")
    |  upper(col("COLUMN_NAME")).like("%_MID%")
    |  upper(col("COLUMN_NAME")).like("%MID_%")
    |  upper(col("COLUMN_NAME")).like("%TITL%")
    |  upper(col("COLUMN_NAME")).like("%SUFFIX%")
    |  upper(col("COLUMN_NAME")).like("%DEPT_NAME%")
    |  upper(col("COLUMN_NAME")).like("%ADDRESS%")
    |  upper(col("COLUMN_NAME")).like("%ADDR1%")
    |  upper(col("COLUMN_NAME")).like("%ADDR2%")
    |  upper(col("COLUMN_NAME")).like("%ORIG_ADDR%")
    |  upper(col("COLUMN_NAME")).like("%ORG_ADDR%")
    |  upper(col("COLUMN_NAME")).like("%ADDR_OR%")
    |  upper(col("COLUMN_NAME")).like("%DEST_ADDR%")
    |  upper(col("COLUMN_NAME")).like("%ADDR_DEST%")
    |  upper(col("COLUMN_NAME")).like("%ADDR_NAGT%")
    |  upper(col("COLUMN_NAME")).like("%STRE_%")
    |  upper(col("COLUMN_NAME")).like("%PHON%")
    |  upper(col("COLUMN_NAME")).like("%ITUDE%")
    |  upper(col("COLUMN_NAME")).like("%_LAT_%")
    |  upper(col("COLUMN_NAME")).like("%_LONG_%")
    |  upper(col("COLUMN_NAME")).like("%IP_ADDRESS%")
    |  upper(col("COLUMN_NAME")).like("%IPADDRESS%")
    |  upper(col("COLUMN_NAME")).like("%EMAIL%")
    |  upper(col("COLUMN_NAME")).like("%E_MAIL%")
)

# Primary Exclusion Conditions
exclude_cond = (
    ~col("DATA_TYPE").isin("DATE","DATETIME","TIMESTAMP","TIMESTAMP_NTZ","TIMESTAMP_LTZ","TIMESTAMP_TZ","BOOLEAN","VARBINARY")
    &  ~upper(col("COLUMN_NAME")).like("%DATE%")
    &  ~upper(col("COLUMN_NAME")).like("%DAYS%")
    &  ~upper(col("COLUMN_NAME")).like("%DLAY_NTFY%")
    &  ~upper(col("COLUMN_NAME")).like("%EMAIL_BOUNCE%")
    &  ~upper(col("COLUMN_NAME")).like("%EMAIL_CLICK%")
    &  ~upper(col("COLUMN_NAME")).like("%EMAIL_DELIVERED%")
    &  ~upper(col("COLUMN_NAME")).like("%EMAIL_OPEN%")
    &  ~upper(col("COLUMN_NAME")).like("%EMAIL_REPLIED%")
    &  ~upper(col("COLUMN_NAME")).like("%INFO%")
    &  ~upper(col("COLUMN_NAME")).like("%LATE%")
    &  ~upper(col("COLUMN_NAME")).like("%LEAD_COORDINATOR%")
    &  ~upper(col("COLUMN_NAME")).like("%MOVE_COORDINATOR%")
    &  ~upper(col("COLUMN_NAME")).like("%OPPORTUNITY%")
    &  ~upper(col("COLUMN_NAME")).like("%OPTY%")
    &  ~upper(col("COLUMN_NAME")).like("%REASON%")
    &  ~upper(col("COLUMN_NAME")).like("%STATUS%")
    &  ~upper(col("COLUMN_NAME")).like("%TYPE%")
    &  ~upper(col("COLUMN_NAME")).like("%_PK%")
    &  ~upper(col("COLUMN_NAME")).like("%_FK%")
    &  ~upper(col("COLUMN_NAME")).like("%_IND")
    &  ~upper(col("COLUMN_NAME")).like("%_ID")
    &  ~upper(col("COLUMN_NAME")).like("Q1%")
    &  ~upper(col("COLUMN_NAME")).like("Q2%")
    &  ~upper(col("COLUMN_NAME")).like("Q3%")
    &  ~upper(col("COLUMN_NAME")).like("Q4%")
    &  ~upper(col("COLUMN_NAME")).like("Q5%")
    &  ~upper(col("COLUMN_NAME")).like("Q6%")
)

# Override previous criteria to EXCLUDE specified column names
remove_cond = col("COLUMN_NAME").isin(
    "AGCY_LEAD_COORD_EMAIL",
    "AGENTEMAILSTATUS",
    "BILL_TO_AGNT_NBR",
    "BILL_TO_TYPE", 
    "BOKR_EMAIL",
    "CARRIER_EMAIL",
    "CUSTOMEREMAILSTATUS",
    "CUSTOMER_DIM_FK",
    "EMAIL_TC",
    "G11_AUTH_INIT",
    "ID_RANK_SHPR",
    "INDT_CONT_DEST",
    "INDT_FLR_FLAT",
    "INDT_NTFY_GBNR",
    "INDT_SHPR_NTFY",
    "LOC_ADDR1_WHSE_DEST",
    "LOC_ADDR2_WHSE_DEST",
    "LOC_ADDR1_WHSE_ORG",
    "LOC_ADDR2_WHSE_ORG",
    "MOVE_MNGR_EMAIL",
    "ST_ISTL_MILI", 
    "UNI_VOICEMAIL_DESC",
    "ZIP_DIVS",
)

# Override previous criteria to ADD BACK specified column names
override_cond = (
    (col("COLUMN_NAME") == "COMPANY") | 
    (col("COLUMN_NAME") == "CONTRACT_NAME") |
    (col("COLUMN_NAME") == "DEALNAME") |
    (col("COLUMN_NAME") == "MOVE_PRTY_MSG_TEXT") |
    (col("COLUMN_NAME") == "NAME_ACCT") |
    (col("COLUMN_NAME") == "REMOTENUMBERCALLID") |
    (col("COLUMN_NAME") == "SMS_ADDR_DESC")
)

# -----------------------------------------------------------------------------
# SECTION 2: IDENTIFY EMAIL COLUMNS THAT CONTAIN THE SUBMITTED ADDRESS
# -----------------------------------------------------------------------------

def search_and_store_email_locations(session: Session,
                                     email_address: str,
                                     db_list: list[str] = search_databases):
    print("üîé Scanning databases for", email_submitted, db_list)
    target = email_address.upper()
    hits: list[tuple[str,str,str,str]] = []
    table_counts: dict[str,int] = {}

    for db in db_list:
        # Look only at base tables
        tables_meta = (
            session.table(f"{db}.INFORMATION_SCHEMA.TABLES")
                   .filter(col("TABLE_TYPE") == "BASE TABLE")
                   .select("TABLE_SCHEMA","TABLE_NAME")
        )

        # Find any columns whose name looks like email
        email_cols = (
            session.table(f"{db}.INFORMATION_SCHEMA.COLUMNS")
                   .join(tables_meta, ["TABLE_SCHEMA","TABLE_NAME"], how="inner")
                   .filter(
                       upper(col("COLUMN_NAME")).like("%EMAIL%")
                    |  upper(col("COLUMN_NAME")).like("%E_MAIL%")
                   )
                   .select(
                       lit(db).alias("DATABASE_NAME"),
                       col("TABLE_SCHEMA"),
                       col("TABLE_NAME"),
                       col("COLUMN_NAME")
                   )
        ).collect()

        for row in email_cols:
            d, s, t, c = row["DATABASE_NAME"], row["TABLE_SCHEMA"], row["TABLE_NAME"], row["COLUMN_NAME"]
            table_id = f"{d}.{s}.{t}"

            # get and cache total row count
            if table_id not in table_counts:
                try:
                    table_counts[table_id] = session.table(table_id).count()
                except Exception:
                    table_counts[table_id] = None

            try:
                tbl = session.table(table_id)
                match_count = tbl.filter(upper(col(c)).like(f"%{target}%")).count()
            except Exception:
                continue

            if match_count > 0:
                total_rows = table_counts[table_id]
                if total_rows is not None:
                    print(f"‚ÑπÔ∏è {table_id}.{c}: {match_count} of {total_rows} rows match the submitted email")
                else:
                    print(f"‚ÑπÔ∏è {table_id}.{c}: {match_count} matching rows")

                hits.append((d, s, t, c))

    if not hits:
        print(f"‚ÑπÔ∏è No matching email values found for {email_address} in {db_list} ‚Äî stopping.")
        return []

    # write out only the hit-columns
    rows = [
        {"DATABASE_NAME": d, "TABLE_SCHEMA": s, "TABLE_NAME": t, "COLUMN_NAME": c}
        for d, s, t, c in hits
    ]
    result_df = (
        session.create_dataframe(rows)
               .with_column("SEARCHED_EMAIL", lit(email_address))
               .with_column("SEARCH_TIMESTAMP", current_timestamp())
    )
    # Append records to Snowflake table
    result_df.write.mode("append").save_as_table("PRD_MART.DATA_ANONYMIZATION.EMAIL_SEARCH_RESULTS")
    return hits

# -----------------------------------------------------------------------------
# SECTION 3: GRANT PRIVILEGES TO SYSADMIN (restricted to Section 2 results)
# -----------------------------------------------------------------------------

def grant_privileges(session: snowpark.Session):
    """
    Grant USAGE and UPDATE privileges only on the databases, schemas, and tables
    where the submitted email was actually found (per EMAIL_SEARCH_RESULTS),
    restricted to the configured `search_databases`.
    """
    print(" ")
    print("üîê Granting scoped privileges based on EMAIL_SEARCH_RESULTS...")
    print("üîê Restricting to databases:", search_databases)

    # Load and filter results: only submitted email AND search_databases
    results_df = (
        session.table("PRD_MART.DATA_ANONYMIZATION.EMAIL_SEARCH_RESULTS")
        .filter(col("SEARCHED_EMAIL") == lit(email_submitted))
        .filter(col("DATABASE_NAME").isin(search_databases))
        .select("DATABASE_NAME", "TABLE_SCHEMA", "TABLE_NAME")
        .distinct()
    )

    if results_df.count() == 0:
        print(f"‚ÑπÔ∏è No matching entries for {email_submitted} in {search_databases}; nothing to grant.")
        return

    # Grant USAGE on each database
    dbs = [row["DATABASE_NAME"] for row in results_df.select("DATABASE_NAME").distinct().collect()]
    for db in dbs:
        try:
            session.sql(f"GRANT USAGE ON DATABASE {db} TO ROLE SYSADMIN").collect()
        except Exception as e:
            print(f"‚ö†Ô∏è Failed to grant USAGE on database {db}: {e}")

    # Grant USAGE on each schema
    schemas = results_df.select("DATABASE_NAME", "TABLE_SCHEMA").distinct().collect()
    for row in schemas:
        schema_id = f"{row['DATABASE_NAME']}.{row['TABLE_SCHEMA']}"
        try:
            session.sql(f"GRANT USAGE ON SCHEMA {schema_id} TO ROLE SYSADMIN").collect()
        except Exception as e:
            print(f"‚ö†Ô∏è Failed to grant USAGE on schema {schema_id}: {e}")

    # Grant UPDATE on each table
    tables = results_df.collect()
    for row in tables:
        table_id = f"{row['DATABASE_NAME']}.{row['TABLE_SCHEMA']}.{row['TABLE_NAME']}"
        try:
            session.sql(f"GRANT UPDATE ON TABLE {table_id} TO ROLE SYSADMIN").collect()
        except Exception as e:
            print(f"‚ö†Ô∏è Failed to grant UPDATE on table {table_id}: {e}")

# -----------------------------------------------------------------------------
# SECTION 4: CLASSIFY PII-RELATED COLUMNS
# -----------------------------------------------------------------------------

def classify_columns(session: snowpark.Session):
    print(" ")
    print("üîé Classifying columns and counting total columns per table:")

    # Load filtered tables
    filtered_tables_df = (
        session.table("PRD_MART.DATA_ANONYMIZATION.EMAIL_SEARCH_RESULTS")
        .filter(col("SEARCHED_EMAIL") == lit(email_submitted))
        .filter(col("DATABASE_NAME").isin(search_databases))
        .select("DATABASE_NAME", "TABLE_SCHEMA", "TABLE_NAME")
        .distinct()
    )

    filtered_count = filtered_tables_df.count()
    print(f"‚ÑπÔ∏è Tables matching '{email_submitted}' in {search_databases}: {filtered_count}")
    if filtered_count == 0:
        print("‚ÑπÔ∏è No tables found for this email ‚Äî skipping classification.")
        return

    # Collect restricted list of tables
    tables = [(r["DATABASE_NAME"], r["TABLE_SCHEMA"], r["TABLE_NAME"]) 
              for r in filtered_tables_df.collect()]

    meta_frames = []
    for db, schema, tbl in tables:
        try:
            # Fetch only the exact columns for the tables identified in Section 2
            cols = (session.table(f"{db}.INFORMATION_SCHEMA.COLUMNS")
                .filter((col("TABLE_SCHEMA") == lit(schema)) & (col("TABLE_NAME") == lit(tbl)))
                .select(
                    lit(db).alias("DATABASE_NAME"),
                    col("TABLE_SCHEMA"),
                    col("TABLE_NAME"),
                    col("COLUMN_NAME"),
                    col("DATA_TYPE"),
                    col("CHARACTER_MAXIMUM_LENGTH")
                )
            )

            meta_frames.append(cols)
        except Exception as e:
            print(f"‚ö†Ô∏è Could not fetch metadata for {db}.{schema}.{tbl}: {e}")

    if not meta_frames:
        print("‚ö†Ô∏è No metadata retrieved ‚Äî aborting classification.")
        return

    # Union metadata for the filtered tables
    meta = reduce(lambda a, b: a.union(b), meta_frames) if meta_frames else None
    if meta is None:
        print("‚ö†Ô∏è No metadata retrieved ‚Äî aborting classification.")
        return

    # Apply PII classification masks
    regular_mask  = filter_cond & exclude_cond & ~remove_cond
    override_mask = override_cond
    pii_mask_final = regular_mask | override_mask

    # Apply the unified mask to get detailed PII columns
    detailed_pre_df = meta.filter(pii_mask_final)

    # Build a map of PII columns per table for printing
    pii_map = dd(list)
    for r in detailed_pre_df.select(
            "DATABASE_NAME", "TABLE_SCHEMA", "TABLE_NAME", "COLUMN_NAME"
        ).collect():
        pii_map[(r["DATABASE_NAME"], r["TABLE_SCHEMA"], r["TABLE_NAME"])].append(r["COLUMN_NAME"])

    # Compute total vs. PII counts
    flagged = meta.with_column("IS_PII", pii_mask_final)
    summary_df = (
        flagged
          .group_by("DATABASE_NAME", "TABLE_SCHEMA", "TABLE_NAME")
          .agg(
            count("*").alias("TOTAL_COLUMNS"),
            sf_sum(when(col("IS_PII"), 1).otherwise(0)).alias("PII_COLUMNS")
          )
    )
    full_summary = (
        filtered_tables_df
          .join(summary_df, ["DATABASE_NAME", "TABLE_SCHEMA", "TABLE_NAME"], how="left")
          .select(
            "DATABASE_NAME", "TABLE_SCHEMA", "TABLE_NAME",
            coalesce(col("TOTAL_COLUMNS"), lit(0)).alias("TOTAL_COLUMNS"),
            coalesce(col("PII_COLUMNS"),   lit(0)).alias("PII_COLUMNS")
          )
    )

    # Print the classification summary
    for r in full_summary.collect():
        db, schema, tbl = r["DATABASE_NAME"], r["TABLE_SCHEMA"], r["TABLE_NAME"]
        tot, pii_ct     = r["TOTAL_COLUMNS"], r["PII_COLUMNS"]
        cols = sorted(pii_map.get((db, schema, tbl), []))
        print(f"‚ÑπÔ∏è {db}.{schema}.{tbl}: {pii_ct} of {tot} columns are PII ‚Üí {cols}")

    # Persist detailed classification
    detailed_df = (
        detailed_pre_df
          .select(
            "DATABASE_NAME", "TABLE_SCHEMA", "TABLE_NAME", "COLUMN_NAME",
            when(
                (upper(col("COLUMN_NAME")).like("%EMAIL%")) |
                (upper(col("COLUMN_NAME")).like("%E_MAIL%")),
                lit("EMAIL")
            )
            .when(col("DATA_TYPE") == "NUMBER", lit("NUMBER"))
            .when(
                col("CHARACTER_MAXIMUM_LENGTH").isNotNull()
                & (col("CHARACTER_MAXIMUM_LENGTH") <= 7),
                lit("SHORT_TEXT")
            )
            .when(
                col("CHARACTER_MAXIMUM_LENGTH").isNull()
                | (col("CHARACTER_MAXIMUM_LENGTH") > 7),
                lit("LONG_TEXT")
            )
            .otherwise(col("DATA_TYPE"))
            .alias("DATA_TYPE")
          )
          .with_column("SUBMITTED_EMAIL", lit(email_submitted))
          .with_column("CLASSIFY_TIMESTAMP", current_timestamp())
    )

    # Write dataframe to Snowflake
    detailed_df.write.mode("append").save_as_table("PRD_MART.DATA_ANONYMIZATION.DETAILED_COLUMN_CLASSIFICATION_LOG")
    print("üì• Written detailed PII-column log to DETAILED_COLUMN_CLASSIFICATION_LOG")

# -----------------------------------------------------------------------------
# SECTION 5: APPLY NON-EMAIL ANONYMIZATION   
# -----------------------------------------------------------------------------

def apply_nonemail_anonymization(session: Session):
    """
    Mask NUMBER, SHORT_TEXT, and LONG_TEXT columns in tables
    where the submitted email was found ‚Äî filtering on the email columns.
    """
    print(" ")
    print("üîÑ Applying anonymization for NUMBER, SHORT_TEXT, and LONG_TEXT types...")

    # Load detailed classification log
    class_df = (
        session
        .table("PRD_MART.DATA_ANONYMIZATION.DETAILED_COLUMN_CLASSIFICATION_LOG")
        .filter(col("SUBMITTED_EMAIL") == lit(email_submitted))
        .filter(col("DATABASE_NAME").isin(search_databases))
        .select("DATABASE_NAME", "TABLE_SCHEMA", "TABLE_NAME", "COLUMN_NAME", "DATA_TYPE")
    )

    # Find which tables/columns contained the submitted email
    search_rows = (
        session
        .table("PRD_MART.DATA_ANONYMIZATION.EMAIL_SEARCH_RESULTS")
        .filter(col("SEARCHED_EMAIL") == lit(email_submitted))
        .select("DATABASE_NAME", "TABLE_SCHEMA", "TABLE_NAME", "COLUMN_NAME")
        .collect()
    )

    # Map each table to its email‚Äêcolumns
    email_cols_map: dict[tuple[str,str,str], list[str]] = dd(list)
    for r in search_rows:
        key = (r["DATABASE_NAME"], r["TABLE_SCHEMA"], r["TABLE_NAME"])
        email_cols_map[key].append(r["COLUMN_NAME"])

    # Group columns by table for only for tables with submitted_email 
    table_cols: dict[tuple[str,str,str], list[tuple[str,str]]] = dd(list)
    for row in class_df.collect():
        key = (row["DATABASE_NAME"], row["TABLE_SCHEMA"], row["TABLE_NAME"])
        dt = row["DATA_TYPE"].upper()
        if dt in ("NUMBER", "SHORT_TEXT", "LONG_TEXT") and key in email_cols_map:
            table_cols[key].append((row["COLUMN_NAME"], dt))

    # For each affected table, execute a Table.update
    from functools import reduce
    for (db, schema, tbl), cols in table_cols.items():
        table_id = f"{db}.{schema}.{tbl}"
        tbl_obj = session.table(table_id)

        # Build a case‚Äêinsensitive WHERE condition on the email columns
        email_conds = [
            lower(trim(col(email_col))) == lower(lit(email_submitted))
            for email_col in email_cols_map[(db, schema, tbl)]
        ]
        where_cond = reduce(lambda a, b: a | b, email_conds)

        # Build the assignments dict for update()
        assignments = {
            col_name: (
                lit(0) if dt == "NUMBER"
                else lit("") if dt == "SHORT_TEXT"
                else lit("Removed") if dt == "LONG_TEXT"
                else lit("")
            )
            for col_name, dt in cols
        }
        
        try:
            # ‚úÖ call Table.update(assignments, condition)
            tbl_obj.update(assignments, where_cond)
            col_list = ", ".join(c for c, _ in cols)
            print(f"‚úÖ {table_id}: anonymized columns ‚Üí {col_list}")
        except Exception as e:
            print(f"‚ö†Ô∏è Failed to anonymize {table_id}: {e}")

# -----------------------------------------------------------------------------
# SECTION 6: APPLY EMAIL ANONYMIZATION 
# -----------------------------------------------------------------------------

def apply_only_email_anonymization(session: Session):
    """
    Mask EMAIL columns in tables where the submitted email was found.
    Reports how many rows match before & after, so you can spot leftovers.
    """
    
    print("\nüîÑ Applying anonymization for EMAIL types‚Ä¶")
    target = email_submitted.upper()

    # Load your classification log
    class_df = (
        session
        .table("PRD_MART.DATA_ANONYMIZATION.DETAILED_COLUMN_CLASSIFICATION_LOG")
        .filter(col("SUBMITTED_EMAIL") == lit(email_submitted))
        .filter(col("DATABASE_NAME").isin(search_databases))
        .select("DATABASE_NAME","TABLE_SCHEMA","TABLE_NAME","COLUMN_NAME","DATA_TYPE")
    )

    # Find which tables actually contained the email
    search_rows = (
        session
        .table("PRD_MART.DATA_ANONYMIZATION.EMAIL_SEARCH_RESULTS")
        .filter(col("SEARCHED_EMAIL") == email_submitted)
        .select("DATABASE_NAME","TABLE_SCHEMA","TABLE_NAME","COLUMN_NAME")
        .collect()
    )

    # Build a map of email-columns *that you also tagged as EMAIL*
    email_cols_map = dd(list)
    for r in search_rows:
        key = (r["DATABASE_NAME"], r["TABLE_SCHEMA"], r["TABLE_NAME"])
        # confirm it‚Äôs in the detailed log as DATA_TYPE = 'EMAIL'
        is_email = (
            class_df
              .filter(
                  (col("DATABASE_NAME")==r["DATABASE_NAME"]) &
                  (col("TABLE_SCHEMA")==r["TABLE_SCHEMA"]) &
                  (col("TABLE_NAME" )==r["TABLE_NAME" ]) &
                  (col("COLUMN_NAME")==r["COLUMN_NAME"]) &
                  (col("DATA_TYPE"  )=="EMAIL")
              )
              .count() > 0
        )
        if is_email:
            email_cols_map[key].append(r["COLUMN_NAME"])

    # For each table, do the LIKE-based update and diagnostics
    for (db, schema, tbl), cols in email_cols_map.items():
        table_id = f"{db}.{schema}.{tbl}"
        tbl_obj  = session.table(table_id)

        # build a substring‚Äêmatch condition across all email columns
        email_conds = [
            upper(trim(col(c))).like(f"%{target}%")
            for c in cols
        ]
        where_cond = reduce(lambda a, b: a | b, email_conds)

        # How many rows *before*?
        pre_count = tbl_obj.filter(where_cond).count()
        print(f"üîç {table_id}: {pre_count} rows match before anonymization")

        if pre_count == 0:
            print(f"   ‚Ä¢ skipping (no matching rows)")
            continue

        # Perform the update
        assignments = {c: lit("Removed") for c in cols}
        try:
            tbl_obj.update(assignments, where_cond)
            print(f"‚úÖ {table_id}: anonymized columns ‚Üí {', '.join(cols)}")
        except Exception as e:
            print(f"‚ö†Ô∏è FAILED to anonymize {table_id}: {e}")
            continue

        # Confirm how many rows *after* still match
        post_count = tbl_obj.filter(where_cond).count()
        if post_count > 0:
            print(f"‚ö†Ô∏è {table_id}: {post_count} rows still contain the email after update")
        else:
            print(f"   ‚Ä¢ all rows cleared (0 remaining)")

    print("üîî EMAIL anonymization pass complete.\n")
    
# -----------------------------------------------------------------------------
# SECTION 7: MAIN FUNCTION ENTRY POINT
# -----------------------------------------------------------------------------

def main():
    session = snowpark.Session.builder.configs(connection_parameters).create()
    hits = search_and_store_email_locations(session, email_submitted, search_databases)
    if not hits:
        print(" ")
        print("üîî Submitted email not found in the list of databases ‚Äî nothing to anonymize.")
        print(" ")
        return
    grant_privileges(session)
    classify_columns(session)
    apply_nonemail_anonymization(session)
    apply_only_email_anonymization(session)
    print(" Anonymization process completed.")
    
if __name__ == "__main__":
    main()
